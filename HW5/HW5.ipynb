{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Probabilistic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T07:28:51.276629Z",
     "start_time": "2020-04-13T07:28:51.273696Z"
    }
   },
   "source": [
    "Student Name: Kuan-Lin Liu\n",
    "\n",
    "Student ID: kll482"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "### 1.1 Equivalence of ERM and probabilistic approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)\n",
    "\n",
    "$ERM = argmin_w \\frac{1}{n}\\sum_{i=1}^n log[1+exp(-y_iw^Tx_i)]$\n",
    "\n",
    "$\\hat{R}(w) = \\frac{1}{n}\\sum_{i=1}^n log[1+exp(-y_iw^Tx_i)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T07:29:22.989569Z",
     "start_time": "2020-04-13T07:29:22.982091Z"
    }
   },
   "source": [
    "(b)\n",
    "\n",
    "Let $$P(y=1|x;w) = f(w^Tx) = \\frac{1}{1+exp(-w^Tx)}$$\n",
    "\n",
    "We know $$\\enspace P(Y=y|X=x)=f(w^Tx)^y[1-f(w^Tx)]^{(1-y)}$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$L(w) = \\prod_{i=1}^n P(Y=y_i|X=x_i) = \\prod_{i=1}^n f(w^Tx)^y[1-f(w^Tx)]^{(1-y)} $$\n",
    "$$LL(w) = \\log\\left[ \\prod_{i=1}^n f(w^Tx)^y[1-f(w^Tx)]^{(1-y)} \\right] = \\sum_{i=1}^n y_i\\log f(w^Tx_i) + (1-y_i)\\log (1-f(w^Tx_i)) $$\n",
    "$$NLL(w) = -\\sum_{i=1}^n y_i\\log f(w^Tx_i) + (1-y_i)\\log (1-f(w^Tx_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\n",
    "\n",
    "Prove (a) and (b) are equal\n",
    "\n",
    "When $\\enspace y_i=1 \\enspace\\text{and} \\enspace \\hat{y_i} = 1,$\n",
    "$$NLL(w) = \\sum_{i=1}^n-\\log f(w^Tx_i) = \\sum_{i=1}^n\\log (1+exp(-w^Tx_i)) = n\\hat{R(w)}$$\n",
    "\n",
    "When $\\enspace y_i=-1 \\enspace\\text{and} \\enspace \\hat{y_i} = 0,$\n",
    "\n",
    "$$NLL(w) = \\sum_{i=1}^n-\\log (1-f(w^Tx_i))$$\n",
    "\n",
    "$$= \\sum_{i=1}^n \\log (1-\\frac{1}{1+exp(-w^Tx_i)})^{-1}$$\n",
    "\n",
    "$$= \\sum_{i=1}^n \\log (\\frac{exp(-w^Tx_i)}{1+exp(-w^Tx_i)})^{-1} $$\n",
    "\n",
    "$$= \\sum_{i=1}^n \\log (\\frac{1+exp(-w^Tx_i)}{exp(-w^Tx_i)})$$\n",
    "\n",
    "$$= \\sum_{i=1}^n \\log (1+exp(w^Tx_i))$$\n",
    "\n",
    "$$= n\\hat{R(w)}$$\n",
    "\n",
    "Since n is a constant, ERM and MLE will not be affected by the constant and will produce the same w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Linearly Separable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1\n",
    "\n",
    "According to the condition of that the data is linearly separable, we can find a decision boundary that predict y=1 if $w^Tx \\geq 0$ and y=0 if $w^Tx < 0$. The decision boundary is $w^Tx = 0$ with $P(y=1|x;w)=0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2\n",
    "\n",
    "$\\frac{\\partial NLL(w; c)}{\\partial c} = \\frac{\\partial -\\sum_{i=1}^n y_i\\log f(cw^Tx_i) + (1-y_i)\\log (1-f(cw^Tx_i))}{\\partial c}$\n",
    "\n",
    "Let $z_i = cw^Tx_i$ and $f_i=f(cw^Tx_i)$,\n",
    "\n",
    "$\\frac{\\partial NLL(w; c)}{\\partial c}=\\frac{\\partial NLL}{\\partial f_i} \\cdot \\frac{\\partial f_i}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial c}$\n",
    "\n",
    "$=-\\sum_{i=1}^n [\\frac{y_i}{f_i} - \\frac{1-y}{1-f}]\\cdot f_i(1-f_i) \\cdot w^Tx_i$\n",
    "\n",
    "$=-\\sum_{i=1}^n[(1-f_i)y_i-f_i(1-y_i)] \\cdot w^Tx_i$\n",
    "\n",
    "$=-\\sum_{i=1}^n [y_i-f_i] \\cdot w^Tx_i$\n",
    "\n",
    "$=\\sum_{i=1}^n [f_i-y_i] \\cdot w^Tx_i$\n",
    "\n",
    "$=\\sum_{i=1}^n [f_i(cw^Tx)-y_i] \\cdot w^Tx_i$\n",
    "\n",
    "$\\because \\text{If} \\ c \\rightarrow \\infty, \\ f(cw^Tx_i) \\rightarrow 1$\n",
    "\n",
    "$\\therefore$ the derivative of NLL on c is strictly positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1\n",
    "\n",
    "First, let's prove $\\sum_{i=1}^n log(1+exp(y_iw^Tx_i)$ is convex.\n",
    "\n",
    "\n",
    "Since we know $\\sum_{i=1}^n \\lambda ||w||^2$ is also convex, so $J_{logistic} (w)$ is also convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:43:58.563745Z",
     "start_time": "2020-04-13T09:43:58.358594Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T22:15:15.518341Z",
     "start_time": "2020-04-14T22:15:15.513917Z"
    }
   },
   "outputs": [],
   "source": [
    "def f_objective(theta, X, y, l2_param=1):\n",
    "    '''\n",
    "    Args:\n",
    "        theta: 1D numpy array of size num_features\n",
    "        X: 2D numpy array of size (num_instances, num_features)\n",
    "        y: 1D numpy array of size num_instances\n",
    "        l2_param: regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        objective: scalar value of objective function\n",
    "    '''\n",
    "    # y should be in {-1, 1}\n",
    "    n = X.shape[0]\n",
    "    x1 = 0 # exp(0) = 1\n",
    "    x2 = np.array([-y[i]*v for i, v in enumerate(np.dot(X, theta))])\n",
    "    return np.sum(np.logaddexp(x1, x2))/n + l2_param*np.sum(np.square(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:15:40.870618Z",
     "start_time": "2020-04-15T13:15:40.866793Z"
    }
   },
   "outputs": [],
   "source": [
    "def f_objective(theta, X, y, l2_param=1):\n",
    "    n = X.shape[0]\n",
    "    summ = 0\n",
    "    for i in range(n):\n",
    "        summ += np.logaddexp(0, -y[i]*np.dot(theta, X[i]))\n",
    "    return summ/n + l2_param*sum(theta**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:28:16.855433Z",
     "start_time": "2020-04-15T13:28:16.851957Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_logistic_reg(X, y, objective_function, l2_param=1):\n",
    "    '''\n",
    "    Args:\n",
    "        X: 2D numpy array of size (num_instances, num_features)\n",
    "        y: 1D numpy array of size num_instances\n",
    "        objective_function: function returning the value of the objective\n",
    "        l2_param: regularization parameter\n",
    "        \n",
    "    Returns:\n",
    "        optimal_theta: 1D numpy array of size num_features\n",
    "    '''\n",
    "    theta_0 = np.zeros(X.shape[1])\n",
    "    minimizer = minimize(objective_function, theta_0, args=(X, y, l2_param))\n",
    "    \n",
    "    # return the optimal theta\n",
    "    return minimizer.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember to do preprocessing and add the bias vector in the following question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:15:46.212594Z",
     "start_time": "2020-04-15T13:15:46.208867Z"
    }
   },
   "outputs": [],
   "source": [
    "### function for reading the txt file ###\n",
    "def read_txt(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for row in f:\n",
    "            row_float = [float(i) for i in row.strip().split(\",\")]\n",
    "            data.append(row_float)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:20:04.733189Z",
     "start_time": "2020-04-15T13:20:04.699364Z"
    }
   },
   "outputs": [],
   "source": [
    "### input X_train, X_val, y_train, y_val ###\n",
    "relative_path = \"code/logistic-code/\"\n",
    "X_train = read_txt(relative_path+\"X_train.txt\")\n",
    "X_val = read_txt(relative_path+\"X_val.txt\")\n",
    "y_train = read_txt(relative_path+\"y_train.txt\")\n",
    "y_val = read_txt(relative_path+\"y_val.txt\")\n",
    "\n",
    "### revise the domain of y_train and y_val to be in {-1, 1} ###\n",
    "y_train = y_train.reshape(y_train.shape[0])\n",
    "y_train = [1 if row > 0.5 else -1 for row in y_train]\n",
    "\n",
    "y_val = y_val.reshape(y_val.shape[0])\n",
    "y_val = [1 if row > 0.5 else -1 for row in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:20:04.905302Z",
     "start_time": "2020-04-15T13:20:04.901578Z"
    }
   },
   "outputs": [],
   "source": [
    "### add bias ###\n",
    "bias_train = np.ones((X_train.shape[0], 1))\n",
    "bias_val = np.ones((X_val.shape[0], 1))\n",
    "X_train = np.hstack((X_train, bias_train))\n",
    "X_val = np.hstack((X_val, bias_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:20:05.068744Z",
     "start_time": "2020-04-15T13:20:05.063511Z"
    }
   },
   "outputs": [],
   "source": [
    "### standardize ###\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "X_train_scaled = std_scaler.fit_transform(X_train)\n",
    "X_val_scaled = std_scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:20:06.304162Z",
     "start_time": "2020-04-15T13:20:05.475922Z"
    }
   },
   "outputs": [],
   "source": [
    "### fit ###\n",
    "weight = fit_logistic_reg(X_train_scaled, y_train, f_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:20:06.364715Z",
     "start_time": "2020-04-15T13:20:06.360999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.55816228e-04, -2.98337651e-04,  3.02827133e-03,  1.05326768e-01,\n",
       "       -3.58817577e-03, -1.35868486e-03, -3.85249746e-03, -7.90075397e-04,\n",
       "       -1.14383982e-03, -7.17818640e-02,  6.54805982e-03, -4.51114274e-03,\n",
       "        1.12493000e-02, -3.86482133e-03, -2.71206254e-03,  1.50358193e-03,\n",
       "       -2.78418333e-03, -9.19055703e-03, -6.82303734e-03, -1.02758348e-02,\n",
       "       -9.05911182e-09])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the question 1.1 above, we have proved $NLL(w)$ is equal to $\\hat{nR(w)}$. Therefore, $LL(w)$ is equal to $-\\hat{nR(w)}$. Just for reminding, $\\hat{R(w)}$ is my objective function without the regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:27:13.051563Z",
     "start_time": "2020-04-15T13:27:13.048259Z"
    }
   },
   "outputs": [],
   "source": [
    "### get loglikelilood ###\n",
    "def get_logll_function(X, y, theta, l2_param):\n",
    "    my_objective = f_objective(theta, X, y, l2_param)\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    return -n*(my_objective-l2_param*np.sum(theta**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:27:14.950654Z",
     "start_time": "2020-04-15T13:27:14.946969Z"
    }
   },
   "outputs": [],
   "source": [
    "### function for plotting the LL for a list of regularization parameter ###\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_logll(l2_param_list, logll_list):\n",
    "    plt.plot(l2_param_list, logll_list, marker=\"o\")\n",
    "    plt.xlabel(\"l2 param\")\n",
    "    plt.ylabel(\"log likelihood\")\n",
    "    plt.title(\"The curve of log likelihood\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:28:51.213845Z",
     "start_time": "2020-04-15T13:28:51.210284Z"
    }
   },
   "outputs": [],
   "source": [
    "### find the minimal LL ###\n",
    "def train_logistic_reg(X_train, y_train, X_val, y_val, objective_function, l2_param_list):\n",
    "    logll_list = []\n",
    "    for l2 in l2_param_list:\n",
    "        best_theta = fit_logistic_reg(X_train, y_train, objective_function, l2) \n",
    "        logll_list.append(get_logll_function(X_val, y_val, best_theta, l2))\n",
    "    \n",
    "    # return the logll list\n",
    "    return logll_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:30:06.692615Z",
     "start_time": "2020-04-15T13:29:15.097846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### plot the result ###\n",
    "l2_param_list = np.linspace(0.001, 0.1, 50)\n",
    "logll_list_result = train_logistic_reg(X_train_scaled, y_train, X_val_scaled, y_val, f_objective, l2_param_list)\n",
    "plot_logll(l2_param_list, logll_list_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:30:47.924768Z",
     "start_time": "2020-04-15T13:30:47.920870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimal log-likelihood I found is: -233.00235581647405\n",
      "The value of the l2 parameter is: 0.019183673469387756\n"
     ]
    }
   ],
   "source": [
    "### Get maximal log-likelihood from the plot ###\n",
    "max_logll_l2 = max(zip(l2_param_list, logll_list_result), key=lambda x: x[1])\n",
    "print(\"The minimal log-likelihood I found is: {}\".format(max_logll_l2[1]))\n",
    "print(\"The value of the l2 parameter is: {}\".format(max_logll_l2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayesian Logistic Regression with Gaussian Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "\n",
    "$P(w|\\mathcal{D}) = \\frac{P(w\\cap\\mathcal{D})}{P(\\mathcal{D}}=\\frac{P(w)\\cdot P(\\mathcal{D}|w)}{P(\\mathcal{D})} $\n",
    "\n",
    "$\\propto P(w)\\cdot P(\\mathcal{D}|w)$\n",
    "\n",
    "$\\propto P(w)\\cdot L(w)$\n",
    "\n",
    "$\\propto P(w)\\cdot exp(LL(w))$ \n",
    "\n",
    "$ \\propto P(w)\\cdot exp(-NLL(w))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "\n",
    "$ L(w) = \\prod_{i=1}^n P(Y=y_i|X=x_i) = \\prod_{i=1}^n f(w^Tx)^y[1-f(w^Tx)]^{(1-y)} $\n",
    "\n",
    "From the equation above, we know the likelihood of logistic regression is a Bernoulli distribution, which is the beta family. However, $w$ has a normal distribution. Therefore, $P(w)$ is not a conjugate prior to the likelihood of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T03:53:04.174687Z",
     "start_time": "2020-04-15T03:53:04.164582Z"
    }
   },
   "source": [
    "Given, $$\\mathcal{N}(0, \\Sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{w^Tw}{2\\Sigma})$$\n",
    "\n",
    "Solving,\n",
    "\n",
    "$$-\\log P(w|\\mathcal{D}) \\propto -\\log[P(w)\\cdot exp(-NLL(w))]$$\n",
    "\n",
    "$$\\propto -\\log P(w) + NLL(w), \\enspace \\text{given} \\ w \\sim \\mathcal{N}(0, \\Sigma)$$\n",
    "\n",
    "$$\\propto -[\\log (2\\pi\\Sigma)^{-\\frac{1}{2}} - \\frac{1}{2}w^Tw\\Sigma^{-1}] + n\\hat{R(w)}$$\n",
    "\n",
    "$$\\propto \\frac{1}{2}log (2\\pi\\Sigma) + \\frac{1}{2}w^Tw\\Sigma^{-1} + n\\hat{R(w)}$$\n",
    "\n",
    "$$\\propto \\frac{1}{2}w^Tw\\Sigma^{-1} + n\\hat{R(w)} \\enspace \\because \\frac{1}{2}log (2\\pi\\Sigma) \\enspace \\text{is constant} $$\n",
    "\n",
    "Let, $$\\Sigma = \\frac{1}{2n\\lambda}I$$\n",
    "\n",
    "Then we can get, $$-\\log P(w|\\mathcal{D}) \\propto n\\hat{R(w)} + n\\lambda ||w||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "\n",
    "Continuing from Q2.3, if $\\Sigma=I$, then we set $\\lambda=\\frac{1}{2n}$ so that the minimizer is equal to the mode of the posterior distribution of w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coin Flipping with Partial Observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T07:33:50.979793Z",
     "start_time": "2020-04-15T07:33:50.970163Z"
    }
   },
   "source": [
    "### 3.1\n",
    "\n",
    "$P(x=H|\\theta_1, \\theta_2) = \\Sigma_{\\acute{z} \\in \\{H, T\\}}P(x=H, z=\\acute{z} \\ |\\theta_1\\theta_2)$\n",
    "\n",
    "Solve,\n",
    "\n",
    "$P(x=H, z=\\acute{z} \\ |\\theta_1\\theta_2)$\n",
    "\n",
    "$= \\frac{P(x=H, z=\\acute{z},\\theta_1\\theta_2)}{P(\\theta_1\\theta_2)}$\n",
    "\n",
    "$=\\frac{P(x=H | z=\\acute{z}, \\  \\theta_1\\theta_2) \\cdot P(z=\\acute{z},\\ \\theta_1\\theta_2)}{P(\\theta_1 \\theta_2)}$\n",
    "\n",
    "$=P(x=H | z=\\acute{z}, \\  \\theta_2) \\cdot P(z=\\acute{z}\\ | \\theta_1\\theta_2) \\enspace \\because \\theta_1 \\ \\text{is independent to} \\ x \\ \\text{given by} \\ z, \\ \\theta_2$\n",
    "\n",
    "$=P(x=H | z=\\acute{z}, \\  \\theta_2) \\cdot P(z=\\acute{z}\\ | \\theta_1) \\enspace \\because \\theta_2 \\ \\text{is independent to z given by } \\theta_1$\n",
    "\n",
    "Since $P(x=T|z=T)=1$, we only care about the condition of $\\acute{z}=H$.\n",
    "\n",
    "Therefore, $$P(x=H|\\theta_1, \\theta_2)=P(x=H|z=H, \\theta_2)\\cdot P(z=H|\\theta_1)=\\theta_1\\theta_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "\n",
    "$P(x|\\theta_1, \\theta_2)=P(x=H|\\theta_1, \\theta_2) \\cdot P(x=T|\\theta_1, \\theta_2)$\n",
    "\n",
    "$P(\\mathcal{D_r}|\\theta_1, \\theta_2)=(\\frac{N_r}{n_h+n_t})(\\theta_1\\theta_2)^{n_h}(1-\\theta_1\\theta_2)^{n_t}$\n",
    "\n",
    "$-\\log P(\\mathcal{D_r}|\\theta_1, \\theta_2) \\propto -n_h \\log \\theta_1\\theta_2 - n_t \\log (1-\\theta_1\\theta_2)$\n",
    "\n",
    "By MLE, we want to minimize $-\\log P(\\mathcal{D}|\\theta_1, \\theta_2)$\n",
    "\n",
    "(a) \n",
    "\n",
    "$\\frac{\\partial -\\log P(\\mathcal{D}|\\theta_1, \\theta_2)}{\\partial \\theta_1}=\\frac{-n_h}{\\theta_1}+\\frac{n_t\\theta_2}{1-\\theta_1\\theta_2}=0$\n",
    "\n",
    "(b)\n",
    "\n",
    "$\\frac{\\partial -\\log P(\\mathcal{D}|\\theta_1, \\theta_2)}{\\partial \\theta_2}=\\frac{-n_h}{\\theta_2}+\\frac{n_t\\theta_1}{1-\\theta_1\\theta_2}=0$\n",
    "\n",
    "$\\Rightarrow \\frac{n_t\\theta_1\\theta_2}{n_h}=1-\\theta_1\\theta_2$ \n",
    "\n",
    "Substitute $\\frac{n_t\\theta_1\\theta_2}{n_h}$ with $1-\\theta_1\\theta_2$ in (a), we will get $\\frac{-n_h}{\\theta_1}+\\frac{n_hn_t}{n_t\\theta_1}=0$.\n",
    "\n",
    "So, $\\theta_1, \\theta_2$ can not be estimated using MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:57:56.921284Z",
     "start_time": "2020-04-16T02:57:56.919145Z"
    }
   },
   "source": [
    "### 3.3\n",
    "\n",
    "$P(\\mathcal{D_r, D_c}|\\theta_1, \\theta_2)=P(D_c|\\theta_1)P(D_r|\\theta_1, \\theta_2)$\n",
    "\n",
    "$-\\log P(\\mathcal{D_r, D_c}|\\theta_1, \\theta_2) = -\\log P(D_c|\\theta_1)P(D_r|\\theta_1, \\theta_2)$\n",
    "\n",
    "$\\propto \\big[-c_h \\log \\theta_1 - c_t \\log (1-\\theta_1)\\big]+ \\big[-n_h \\log \\theta_1\\theta_2 - n_t \\log (1-\\theta_1\\theta_2)\\big]$\n",
    "\n",
    "By MLE, we want to minimize $\\propto \\big[-c_h \\log \\theta_1 - c_t \\log (1-\\theta_1)\\big]+ \\big[-n_h \\log \\theta_1\\theta_2 - n_t \\log (1-\\theta_1\\theta_2)\\big]$\n",
    "\n",
    "(a)\n",
    "\n",
    "$\\frac{\\partial -\\log P(\\mathcal{D_r, D_c}|\\theta_1, \\theta_2)}{\\partial \\theta_1}=-\\big[\\frac{c_h}{\\theta_1}-\\frac{c_t}{1-\\theta_1} \\big]-\\big[\\frac{n_h}{\\theta_1}-\\frac{\\theta_2n_t}{1-\\theta_1\\theta_2} \\big]=0$\n",
    "\n",
    "(b)\n",
    "\n",
    "$\\frac{\\partial -\\log P(\\mathcal{D_r, D_c}|\\theta_1, \\theta_2)}{\\partial \\theta_2}=-\\big[\\frac{n_h}{\\theta_2}-\\frac{n_t\\theta_1}{1-\\theta_1\\theta_2}\\big]=0$\n",
    "\n",
    "$\\Rightarrow 1-\\theta_1\\theta_2=\\frac{n_t\\theta_1\\theta_2}{n_h}$\n",
    "\n",
    "Substitute $1-\\theta_1\\theta_2$ with $\\frac{n_t\\theta_1\\theta_2}{n_h}$ in (a), we will get\n",
    "\n",
    "$\\theta_1=\\frac{c_h}{c_h+c_t}$\n",
    "\n",
    "$\\theta_2=\\frac{n_h}{(n_h+n_t)\\theta_1}=\\frac{n_h(c_h+c_t)}{(n_h+n_t)c_h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $g(\\theta_1)=\\theta_1^{h-1}(1-\\theta_1)^{t-1}$\n",
    "\n",
    "$\\theta_{1, MAP}=argmax_{\\theta_1}g(\\theta_1)L(\\theta_1, \\theta_2)$\n",
    "\n",
    "Let $\\acute{L}(\\theta_1, \\theta_2)=g(\\theta_1)L(\\theta_1, \\theta_2)$\n",
    "\n",
    "$\\acute{LL}(\\theta_1, \\theta_2)=\\big[(h-1)\\log \\theta_1+(t-1) \\log (1-\\theta_1) \\big]+ \\big[c_h\\log \\theta_1+c_t \\log(1-\\theta_1) \\big]+ \\big[n_n \\log (\\theta_1, \\theta_2)+n_t \\log(1-\\theta_1, \\theta_2)\\big]$\n",
    "\n",
    "(a)\n",
    "\n",
    "$\\frac{\\partial \\acute{NLL}}{\\partial \\theta_1}=-\\big[\\frac{h-1}{\\theta_1}-\\frac{t-1}{1-\\theta_1} \\big]-\\big[\\frac{n_h}{\\theta_1}-\\frac{\\theta_2n_t}{1-\\theta_1\\theta_2} \\big]-\\big[\\frac{c_h}{\\theta_1}-\\frac{c_t}{1-\\theta_1} \\big]=0$\n",
    "\n",
    "(b)\n",
    "\n",
    "$\\frac{\\partial \\acute{NLL}}{\\partial \\theta_2}=\\frac{n_h}{\\theta_2}-\\frac{\\theta_1n_t}{1-\\theta_1\\theta_2}=0$\n",
    "\n",
    "$\\Rightarrow \\frac{n_h}{\\theta_1n_t}=\\frac{\\theta_2}{1-\\theta_1\\theta_2}$\n",
    "\n",
    "From (a) and (b), we can obtain\n",
    "\n",
    "$\\theta_1=\\frac{c_h+h-1}{c_h+c_t+h+t-2}$\n",
    "\n",
    "$\\theta_2=\\frac{n_h}{(n_h+n_t)\\theta_1}=\\frac{n_h(c_h+c_t+h+t-2)}{(n_h+n_t)(c_h+h-1)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
